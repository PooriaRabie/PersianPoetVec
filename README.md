# Poetry Word Embeddings


## Introduction
This project aims to generate high-quality word embeddings for Persian poetry, focusing on the works of Hafez, Saadi, and Rumi (including Diwan-e Shams and Masnavi). Word embeddings are created using several algorithms—Word2Vec, GloVe, and FastText—to capture the unique semantic relationships and poetic language within these classic texts. Each model is trained on the Persian corpus, resulting in embeddings that reflect the specific cultural and linguistic nuances of Persian poetry.

The embeddings from these custom-trained models are compared with the widely-used, pre-trained models in each algorithm—Google's pre-trained Word2Vec model for English, as well as pre-trained GloVe and FastText models. By contrasting these, we aim to highlight the effectiveness of custom-trained Persian embeddings against more general-purpose English models, demonstrating their potential for applications specific to Persian literature and linguistics.

## **How to Access Large Files in This Repository**

GitHub has a file size limit of 25 MB, so files exceeding this limit are hosted separately on Google Drive. To access all files larger than 25 MB, follow these steps:

1. **Visit the Google Drive Link**  
   Access the complete set of large files by clicking [here](https://drive.google.com/drive/folders/1vBDCuXIhccgEeQkN6YNrK8k_kBmhKbkf?usp=sharing).

2. **Download the Required Files**  
   Browse the Google Drive folder and download the files relevant to your needs.

3. **Place the Files in the Correct Directory**  
   After downloading, place the files into the appropriate directories as specified in the repository's structure or documentation.


## How to Use (Word2Vec)

### Step 1: Download Pre-trained Word2Vec Model

To compare the embeddings generated by this model with the ones trained on the Google News dataset, you can download the pre-trained Word2Vec embeddings from Google's official archive.

1. Visit [Google's Word2Vec archive](https://code.google.com/archive/p/word2vec/).
2. Download the **GoogleNews-vectors-negative300.bin** file from the "Pre-trained word and phrase vectors" section.
3. Once downloaded, place the file into the `Model` directory of this repository.

### Step 2: Set Up the Environment

Make sure you have the required libraries installed:

1. Navigate to the `Requirement` directory in this repository.
2. Install the dependencies using the following command:

   ```bash
   pip install -r requirementsWord2Vec.txt
   ```

After setting everything up, you can start using the Word2Vec embeddings for your project.

## How to Use (GloVe)

### Step 1: Download Pre-trained GloVe Model
To use the pre-trained GloVe embeddings, follow these steps:

1. Visit the [GloVe project page](https://nlp.stanford.edu/projects/glove/).
2. Under the "Download pre-trained word vectors" section, download the **glove.6B.zip** file, which contains embeddings trained on Wikipedia 2014 + Gigaword 5.
3. After downloading, unzip the file and place the resulting **glove.6B.300d.txt** (or any other relevant dimensionality file) into the `Model` directory of this repository.

### Step 2: Set Up the Environment
Make sure you have the required libraries installed:

1. Navigate to the `Requirement` directory in this repository.
2. Install the dependencies using the following command:

   ```bash
   pip install -r requirementsGloVe.txt
   ```

After setting everything up, you can start using the GloVe embeddings for your project.

## How to Use (FastText)

### Step 1: Download Pre-trained FastText Model
To use the pre-trained FastText embeddings, follow these steps:

1. Visit the [FastText official website](https://fasttext.cc/docs/en/crawl-vectors.html).
2. Under the **"Models"** section, find and download the **Persian** model.
3. Choose the `text` (.vec) version of the file for compatibility.
4. After downloading, place the `.vec` file into the `Model` directory of this repository.

### Step 2: Set Up the Environment
Make sure you have the required libraries installed:

1. Navigate to the `Requirements` directory in this repository.

2. Install the dependencies using the following command:

   ```bash
   pip install -r requirementsFastText.txt
   ```
After setting everything up, you can start using the FastText embeddings for your project.






