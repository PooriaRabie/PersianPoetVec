# Poetry Word Embeddings


## Introduction
This project aims to generate high-quality word embeddings for Persian poetry, focusing on the works of Hafez, Saadi, and Rumi (including Diwan-e Shams and Masnavi). Word embeddings are created using several algorithms—Word2Vec, GloVe, and FastText—to capture the unique semantic relationships and poetic language within these classic texts. Each model is trained on the Persian corpus, resulting in embeddings that reflect the specific cultural and linguistic nuances of Persian poetry.

The embeddings from these custom-trained models are compared with the widely-used, pre-trained models in each algorithm—Google's pre-trained Word2Vec model for English, as well as pre-trained GloVe and FastText models. By contrasting these, we aim to highlight the effectiveness of custom-trained Persian embeddings against more general-purpose English models, demonstrating their potential for applications specific to Persian literature and linguistics.

## How to Use (Word2Vec)

### Step 1: Download Pre-trained Word2Vec Model

To compare the embeddings generated by this model with the ones trained on the Google News dataset, you can download the pre-trained Word2Vec embeddings from Google's official archive.

1. Visit [Google's Word2Vec archive](https://code.google.com/archive/p/word2vec/).
2. Download the **GoogleNews-vectors-negative300.bin** file from the "Pre-trained word and phrase vectors" section.
3. Once downloaded, place the file into the `Model` directory of this repository.

### Step 2: Set Up the Environment

Before running the project, make sure you have the necessary libraries installed. You can use `pip` to install the required dependencies:

